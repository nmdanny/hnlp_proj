{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%load_ext autoreload\r\n","%autoreload 2\r\n","\r\n","import bz2\r\n","import pickle\r\n","import sys\r\n","from os import pipe\r\n","from pathlib import Path\r\n","from typing import Tuple\r\n","\r\n","import numpy as np  # noqa: F401\r\n","import pandas as pd  # noqa: F401\r\n","import stanza\r\n","from stanza_batch import batch\r\n","from tqdm.notebook import tqdm \r\n","import toma\r\n","\r\n","from hnlp_proj.delta import DeltaTransformer, create_feature_matrix  # noqa: F401, F403\r\n","from hnlp_proj.loader import (\r\n","    BEN_YEHUDA_STANZA_PICKLE,  # noqa: F401, F403\r\n","    YNET_STANZA_PICKLE,\r\n","    load_ben_yehuda,\r\n","    load_debug,\r\n","    load_eng_test,\r\n","    load_ynet,\r\n",")\r\n","from hnlp_proj.processing import (\r\n","    Processing,  # noqa: F401, F403\r\n","    get_stanza_pipeline,\r\n","    process_data,\r\n",")\r\n","from hnlp_proj.utils import *  # noqa: F401, F403\r\n","from YAP_Wrapper.yap_wrapper.hebtokenizer import num"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def load_dataset(ds_type: str) -> Tuple[pd.DataFrame, Path]:\r\n","    if ds_type == \"ynet\":\r\n","        return load_ynet(), YNET_STANZA_PICKLE\r\n","    if ds_type == \"ben_yehuda\":\r\n","        return load_ben_yehuda(), BEN_YEHUDA_STANZA_PICKLE\r\n","    raise ValueError(f\"Invalid ds_type '{ds_type}'\")\r\n","\r\n","\r\n","df, pickle_path = load_dataset(\"ben_yehuda\")\r\n","\r\n","if pickle_path.exists():\r\n","    raise ValueError(\r\n","        f\"There is already a pickle file at {pickle_path}, please rename it to proceed\"\r\n","    )\r\n","pickle_path.parent.mkdir(parents=True, exist_ok=True)\r\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 6.03MB/s]                    \n","2021-06-07 23:21:12 INFO: Downloading default packages for language: he (Hebrew)...\n","2021-06-07 23:21:14 INFO: File exists: e:\\heb_nlp\\hnlp_proj\\hnlp_proj\\stanza_resources\\he\\default.zip.\n","2021-06-07 23:21:17 INFO: Finished downloading models and saved to e:\\heb_nlp\\hnlp_proj\\hnlp_proj\\stanza_resources.\n","2021-06-07 23:21:17 INFO: Loading these models for language: he (Hebrew):\n","=======================\n","| Processor | Package |\n","-----------------------\n","| tokenize  | htb     |\n","| mwt       | htb     |\n","| pos       | htb     |\n","| lemma     | htb     |\n","=======================\n","\n","2021-06-07 23:21:17 INFO: Use device: gpu\n","2021-06-07 23:21:17 INFO: Loading: tokenize\n","2021-06-07 23:21:20 INFO: Loading: mwt\n","2021-06-07 23:21:20 INFO: Loading: pos\n","2021-06-07 23:21:21 INFO: Loading: lemma\n","2021-06-07 23:21:21 INFO: Done loading processors!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0914bc00a83040599c4472fd4433f0f8","version_major":2,"version_minor":0},"text/plain":["Processing texts via stanza:   0%|          | 0/13408 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Current batch size is 32\n","Processed 1 documents out of 13408\n"]}],"source":["pipeline = get_stanza_pipeline(Processing.StanzaLemma, use_gpu=True)\r\n","numDocs = 0\r\n","docs = []\r\n","\r\n","def tomaFun(batch_size: int, nlp: stanza.Pipeline, data: pd.Series):\r\n","    print(f\"Current batch size is {batch_size}\")\r\n","    for doc in batch(data, nlp, batch_size=batch_size, clear_cache=True):\r\n","        yield doc\r\n","\r\n","try:\r\n","    for doc in tqdm(toma.simple.batch(tomaFun, 32, pipeline, df[\"text\"]), desc=\"Processing texts via stanza\", total=len(df)):\r\n","        if numDocs % 100 == 0:\r\n","            print(f\"Processed {numDocs + 1} documents out of {len(df)}\")\r\n","        numDocs += 1\r\n","        docs.append(doc)\r\n","except Exception as e:\r\n","    print(\r\n","        f\"Got an exception after processing {len(docs)} out of {len(df)}: {e}\",\r\n","        file=sys.stderr,\r\n","    )\r\n","\r\n","# try:\r\n","#     for doc in tqdm(batch(list(df[\"text\"]), pipeline, batch_size=1, clear_cache=True), desc=\"Processing texts via stanza\", total=len(df)):\r\n","#         if numDocs % 100 == 0:\r\n","#             print(f\"Processed {numDocs + 1} documents out of {len(df)}\")\r\n","#         numDocs += 1\r\n","#         docs.append(doc)\r\n","# except Exception as e:\r\n","#     print(\r\n","#         f\"Got an exception after processing {len(docs)} out of {len(df)}: {e}\",\r\n","#         file=sys.stderr,\r\n","#     )\r\n","\r\n","\r\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with bz2.open(pickle_path, \"wb\") as pickle_f:\r\n","    pickle.dump(docs, pickle_f, protocol=4)"]}],"metadata":{"interpreter":{"hash":"ba9d01af4d7b126cd16bf0ccfd6421707c369c630510e5648903ac84ef06ca53"},"kernelspec":{"display_name":"Python 3.9.4 64-bit ('hnlp-proj-Q_jjuEk3-py3.9': poetry)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}